{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IOU Tracker\n",
    "\n",
    "IOUTracker implements a tracking algorithm or method to track objects based on their Intersection-Over-Union (IOU) information across the consecutive frames. The core concept of this algorithm refers to the article (http://elvera.nue.tu-berlin.de/files/1517Bochinski2017.pdf). The idea or the assumption is based on an existing and powerful detector and the high frame rate across the consecutive frames. Under this assumption, you can conduct the object tracking with only the localization and the IOU information. The algorithm conducts under a super-high frame rate and provides a foundation for more complicated calculations upon it. \n",
    "\n",
    "On the other hand, such an algorithm requires an evaluation. The evaluation of this implement also refers to two articles, MOT16 benchmark (https://arxiv.org/abs/1603.00831) and Multi-Target Tracker (http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.309.8335&rep=rep1&type=pdf).\n",
    "\n",
    "This implementation uses MOT17Det dataset (https://motchallenge.net/data/MOT17Det/) as an example.\n",
    "\n",
    "* More information please refer to https://github.com/jiankaiwang/ioutracker.\n",
    "* More example videos please refer to ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import time\n",
    "import logging\n",
    "\n",
    "try:\n",
    "  # you must install ioutracker first\n",
    "  from ioutracker import loadLabel, outputAsFramesToVideo, IOUTracker\n",
    "  from ioutracker import EvaluateOnMOTDatasets, ExampleEvaluateMOTDatasets, EvaluateByFrame\n",
    "  logging.warning(\"Load ioutracker from the installed package.\")\n",
    "except Exception as e:\n",
    "  import sys\n",
    "  modulePaths = [os.path.join(\"..\")]\n",
    "  for path in modulePaths: sys.path.append(path)\n",
    "  from ioutracker.dataloaders.MOTDataLoader import loadLabel\n",
    "  from ioutracker.inference.MOTDet17Main import outputAsFramesToVideo\n",
    "  from ioutracker.inference.MOTDet17Main import outputAsFramesToVideo\n",
    "  from ioutracker.src.IOUTracker import IOUTracker\n",
    "  from ioutracker.metrics.MOTMetrics import EvaluateOnMOTDatasets, ExampleEvaluateMOTDatasets, EvaluateByFrame\n",
    "  logging.warning(\"Load ioutracker from the relative path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "You can use the shell script under the path, (`./ioutracker/dataloaders/MOTDataDownloader.sh`), in the git repository to download the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUB_DATASET = \"train\"\n",
    "VERSION = \"MOT17Det\"\n",
    "LOCAL_PATH = os.path.join(\"/\", \"tmp\", \"MOT\")\n",
    "\n",
    "# you can change the path pointing to the dataset\n",
    "LABEL_PATH = os.path.join(LOCAL_PATH, \"{}\".format(VERSION + \"Labels\"), SUB_DATASET)\n",
    "assert os.path.exists(LABEL_PATH), \"{} is not found.\".format(LABEL_PATH)\n",
    "\n",
    "# you can change the path pointing to the dataset\n",
    "FRAME_PATH = os.path.join(LOCAL_PATH, \"{}\".format(VERSION), SUB_DATASET)\n",
    "assert os.path.exists(FRAME_PATH), \"{} is not found.\".format(FRAME_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalSamples = next(os.walk(os.path.join(LABEL_PATH)))[1]\n",
    "print(totalSamples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE = \"MOT17-04\"\n",
    "LABEL_FILE_PATH = os.path.join(LABEL_PATH, SAMPLE, \"gt\", \"gt.txt\")\n",
    "assert os.path.exists(LABEL_FILE_PATH), \"{} is not found.\".format(LABEL_FILE_PATH)\n",
    "\n",
    "FRAME_FILE_PATH = os.path.join(FRAME_PATH, SAMPLE, \"img1\")\n",
    "assert os.path.exists(FRAME_FILE_PATH), \"{} is not found.\".format(FRAME_FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output the tracking result on the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FRAME_FPS = {\"MOT17-13\": 25, \"MOT17-11\": 30, \"MOT17-10\": 30, \"MOT17-09\": 30,\n",
    "             \"MOT17-05\": 14, \"MOT17-04\": 30, \"MOT17-02\": 30}\n",
    "assert SAMPLE in list(FRAME_FPS.keys()), \"{} was not found.\".format(SAMPLE)\n",
    "fps = FRAME_FPS[SAMPLE]\n",
    "print(\"Sample {} with FPS: {}\".format(SAMPLE, fps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check whether or not the folder is existing, or create it if it isn't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracking_output = os.path.join(LOCAL_PATH, \"tracking_output\".format(VERSION))\n",
    "if not os.path.exists(tracking_output):\n",
    "  try:\n",
    "    os.mkdir(tracking_output)\n",
    "    print(\"Created the output path: {}\".format(tracking_output))\n",
    "  except Exception as e:\n",
    "    raise Exception(\"Can't create the folder. ({})\".format(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we introduce how to output the tracking result on the consecutive frame to a video file. Notice the flag `plotting` must be set to `True` if you want to output the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputAsFramesToVideo(detection_conf=0.2,\n",
    "                      iou_threshold=0.2,\n",
    "                      min_t=fps,\n",
    "                      track_min_conf=0.5,\n",
    "                      labelFilePath=LABEL_FILE_PATH,\n",
    "                      frameFilePath=FRAME_FILE_PATH,\n",
    "                      trackingOutput=tracking_output,\n",
    "                      fps=fps,\n",
    "                      outputFileName=SAMPLE,\n",
    "                      plotting=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can move the video to the desired path like below.\n",
    "\n",
    "```sh\n",
    "mv /tmp/MOT/tracking_output/tracking_MOT17-04.mp4 ~/Desktop/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple example from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the MOT data first via the API `loadLabel`. You can also use `help` to take a look into the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS, DFPERSONS = loadLabel(src=LABEL_FILE_PATH, format_style=\"metrics_dict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(loadLabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LABELS is a dictionary whose key is the frame ID and whose value is each object detection result. The result keeps the localization and visibility in a list, more detail is `[x, y, w, h, visibility]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(LABELS[1]), LABELS[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DFPERSONS is a pandas dataframe object that is processed and filtered unnecessary columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DFPERSONS.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can instantiate an IOUTracker to start the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "help(IOUTracker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a ioutracker that implements the IOU tracker algorithm. In this example, we use the default ID increment mechanism to get the tracker ID for each box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "iouTracks = IOUTracker(detection_conf=0.2,\n",
    "                       iou_threshold=0.2,\n",
    "                       min_t=fps,\n",
    "                       track_min_conf=0.5, \n",
    "                       assignedTID=True)\n",
    "\n",
    "differs = 0\n",
    "allFrameIds = list(LABELS.keys())\n",
    "start, end = min(allFrameIds), max(allFrameIds) + 1\n",
    "\n",
    "for frameIdx in range(start, end, 1):\n",
    "  # apply the IOUTracker algorithm\n",
    "  # you can set runPreviousVersion to choose the latest or previous version\n",
    "  # the latest version saves lots of time to return the tracker ID\n",
    "  detectionMapping, finished = iouTracks(LABELS[frameIdx], returnFinishedTrackers=True, runPreviousVersion=False)\n",
    "\n",
    "  if frameIdx % (fps * 5) == 0:\n",
    "    print(\"Frame: {}\".format(frameIdx))\n",
    "    for bboxIdx in range(len(LABELS[frameIdx])):\n",
    "      print(\"BBox: {}, and its Track ID: {}\".format(LABELS[frameIdx][bboxIdx], detectionMapping[bboxIdx][\"tid\"]))\n",
    "    print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable `tid_count` is used to assign the unique ID to each track. \n",
    "\n",
    "In this implementation, the IOUTracker is designed to take object detection results frame by frame, not a whole video. It keeps the information of each track. You can access the active tracks via the `get_active_tracks()` method, and watch the finished tracks via the `get_finished_tracks()` method.\n",
    " \n",
    "On the other hand, you can access the attribute `tid` of each track to get the relative track ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iouTracks = IOUTracker(detection_conf=0.2,\n",
    "                       iou_threshold=0.2,\n",
    "                       min_t=fps,\n",
    "                       track_min_conf=0.5, \n",
    "                       assignedTID=False)\n",
    "\n",
    "tid_count = 1\n",
    "\n",
    "for label in range(1, len(LABELS), 1):\n",
    "  # iou tracker\n",
    "  iouTracks.read_detections_per_frame(detections=LABELS[label])\n",
    "\n",
    "  active_tacks = iouTracks.get_active_tracks()\n",
    "  finished_tracks = iouTracks.get_finished_tracks()\n",
    "\n",
    "  if label % 50 == 0:\n",
    "    print(\"Frame {} tracker Info: active {}, finished {}\".format(label, len(active_tacks), len(finished_tracks)))\n",
    "  \n",
    "  # simple way to assign the tracker ID\n",
    "  for act_track in active_tacks:\n",
    "    if not act_track.tid:\n",
    "      # assign track id to use the color\n",
    "      act_track.tid = tid_count\n",
    "      tid_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ExampleEvaluateMOTDatasets helps you evaluate on each dataset or each video. Here we use the same ground truth data as the predictions to test the functionality due to the lack of a detector.\n",
    "\n",
    "Notice that this step takes a long time to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Predictions, _ = loadLabel(src=LABEL_FILE_PATH, is_path=True, load_Pedestrian=True, load_Static_Person=True,\n",
    "    visible_thresholde=0.2, format_style=\"metrics_dict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "evalMOT = ExampleEvaluateMOTDatasets(LABEL_FILE_PATH, predictions=Predictions, printOnScreen=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otherwise, you can also evaluate the tracking result to the ground truth. The tracking result must contain the tracking ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EvaluateFrameByFrame(groundTruth, predictions, printOnScreen=False):\n",
    "  evalFrame = EvaluateByFrame(requiredTracking=False)  \n",
    "  \n",
    "  gtFKeys = list(groundTruth.keys())\n",
    "  predFKeys = list(predictions.keys())\n",
    "  numTNOnFrame = 0\n",
    "  \n",
    "  ttlFid = list(set(gtFKeys + predFKeys))\n",
    "  ttlFid = sorted(ttlFid)\n",
    "  start = ttlFid[0]\n",
    "  end = ttlFid[-1] + 1\n",
    "  logging.warning(\"start {} and end {}\".format(start, end))\n",
    "  \n",
    "  for fid in tqdm.trange(start, end, 1):\n",
    "    if fid not in gtFKeys and fid not in predFKeys:\n",
    "      # skip the true negative\n",
    "      numTNOnFrame += 1\n",
    "      continue\n",
    "    \n",
    "    GTFrameInfo = groundTruth[fid] if fid in gtFKeys else [[]]\n",
    "    prediction = predictions[fid] if fid in predFKeys else [[]]\n",
    "    \n",
    "    evalFrame.evaluateOnPredsWithTrackerID(GTFrameInfo, prediction)\n",
    "    \n",
    "  metaRes = evalFrame.getMetricsMeta(printOnScreen=printOnScreen)\n",
    "  cmRes = evalFrame.getCM(printOnScreen=printOnScreen)\n",
    "  motaRes = evalFrame.getMOTA(printOnScreen=printOnScreen)\n",
    "  trajRes = evalFrame.getTrackQuality(printOnScreen=printOnScreen)\n",
    "  \n",
    "  if printOnScreen: print(\"The true negatives: {} frames\".format(numTNOnFrame))\n",
    "  \n",
    "  return metaRes, cmRes, motaRes, trajRes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformUID(dicts, dataType=\"int\"):\n",
    "  dictCopy = dicts.copy()\n",
    "  for key in list(dictCopy.keys()):\n",
    "    if dataType in [\"int\", \"integer\"]:\n",
    "      for objIdx in range(len(dictCopy[key])):\n",
    "        dictCopy[key][objIdx][5] = int(dictCopy[key][objIdx][5])\n",
    "    elif dataType in [\"string\", \"str\"]:\n",
    "      for objIdx in range(len(dictCopy[key])):\n",
    "        dictCopy[key][objIdx][5] = str(dictCopy[key][objIdx][5])\n",
    "  return dictCopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groundTruth, _ = loadLabel(src=LABEL_FILE_PATH, is_path=True, load_Pedestrian=True, load_Static_Person=True,\n",
    "    visible_thresholde=0.2, format_style=\"metrics_dict\")\n",
    "\n",
    "predictions, _ = loadLabel(src=LABEL_FILE_PATH, is_path=True, load_Pedestrian=True, load_Static_Person=True,\n",
    "    visible_thresholde=0.2, format_style=\"metrics_dict\")\n",
    "\n",
    "# transform the datatype of UID\n",
    "groundTruth = transformUID(groundTruth, dataType=\"int\")\n",
    "predictions = transformUID(predictions, dataType=\"int\")\n",
    "\n",
    "evalMOT = EvaluateFrameByFrame(groundTruth=groundTruth, predictions=predictions, printOnScreen=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EvaluateOnMOTDatasets class helps you evaluate the multiple datastes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalMOTData = EvaluateOnMOTDatasets()\n",
    "\n",
    "# it is simple to pass the whole package of results into the multiple-dataset evaluator\n",
    "evalMOTData(evalMOT)\n",
    "evalMOTRes = evalMOTData.getResults()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also evaluate metrics on each MOT datasets and then summarize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalMOTData = EvaluateOnMOTDatasets()\n",
    "for key, _ in FRAME_FPS.items():\n",
    "  LABEL_FILE_PATH = os.path.join(LABEL_PATH, key, \"gt\", \"gt.txt\")\n",
    "  assert os.path.exists(LABEL_FILE_PATH), \"{} is not found.\".format(LABEL_FILE_PATH)\n",
    "  print(\"Sample: {}\".format(key))\n",
    "  \n",
    "  # here predictions flag is set to None, it makes to use the ground truth as the prediction\n",
    "  evalMOT = ExampleEvaluateMOTDatasets(LABEL_FILE_PATH, predictions=None, printOnScreen=True)\n",
    "  evalMOTData(evalMOT)\n",
    "  print(\"\", end=\"\\n\\n\")\n",
    "evalMOTRes = evalMOTData.getResults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
